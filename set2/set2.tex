% Claire Goeckner-Wald

\documentclass[10pt,letter]{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{setspace}
\onehalfspacing
\usepackage{fullpage}
\begin{document}

\title{Set 2}
\author{Claire Goeckner-Wald}
\maketitle 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Hoeffding Inequality}

\paragraph{1)} B. 0.01

	See the Hoeffding code. For 100,000 repititions, the average value of $v_{min}$ was about 0.0376.

\paragraph{2)} D. $c_1$ and $c_{rand}$

	See the Hoeffding code. For 100,000 repititions, the average value of $c_1$ and $c_{rand}$ were about 0.5. This satisifies the single-bin Hoeffding Inequality because it 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Error and Noise}

\paragraph{3)} E. $\lambda \mu + (1 - \lambda)(1 - \mu)$

	Function $h$ approximates $f$ with error $\mu$. But there is a $1-\lambda$ chance that $f$ is wrong. We can determine the probability that $h$ incorrectly approximates $y$ with a decision tree. \newpage

\paragraph{4)}

	\begin{align*}
	P(h(x) \neq y) &= \lambda \mu + (1 - \lambda)(1 - \mu) \\
	&= \lambda \mu + (1 - \lambda - \mu + \lambda\mu) \\
	&= 2\lambda \mu - \lambda - \mu + 1 \\
	&= \mu (2\lambda -1) - \lambda + 1 \\
	2\lambda -1 &= 0 \\
	2\lambda &= 1 \\
	\lambda &= \frac{1}{2}
	\end{align*}

	Thus, when $\lambda = \frac{1}{2}$, $\mu$ is irrelevant to the output of $P(h(x) \neq y)$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Linear Regression}

\paragraph{5)} C. 0.01

	See the linear regression code. For 10,000 repititions, the average in-sample error was 0.039185. 

\paragraph{6)} C. 0.01

	See the linear regression code. For 10,000 repititions, the average out-sample error was 0.0484795.

\paragraph{7)}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Nonlinear Transformation}

\paragraph{8)}

\paragraph{9)}

\paragraph{10)}

\end{document}
